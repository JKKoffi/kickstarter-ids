{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge - Kickstarter preprocessing\n",
    "---\n",
    "![](https://images.unsplash.com/photo-1530083727892-3c261661d7a4?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80)\n",
    "Picture by [Steve Halama](https://unsplash.com/photos/GjSzvtZhMoA)\n",
    "\n",
    "In this exercise, we will start working on the Kickstarter dataset, each record is about a specific campaign. Today, you will pre-process the dataset. \n",
    "\n",
    "During the next Spark course, you will apply machine learning to predict successful campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T14:56:44.856792Z",
     "start_time": "2020-03-04T14:56:42.343793Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Failed to create the file train_clean.csv: Permission denied\n",
      "\n",
      "  0 31.1M    0 17046    0     0  13680      0  0:39:45  0:00:01  0:39:44 13702\n",
      "curl: (23) Failed writing body (0 != 16384)\n"
     ]
    }
   ],
   "source": [
    "# Q0 - download csv file\n",
    "!curl -O https://s3.eu-central-1.amazonaws.com/alex-image-hosting/train_clean.csv > train_clean.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1 - Have a look at our train_clean.csv file, with the linux ```head``` command.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:38:26.382409Z",
     "start_time": "2020-03-05T07:38:26.333404Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex‚cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "# Q1 - Unix command for file first lines\n",
    "filename = \"./train_clean.csv\"\n",
    "!head - n 2 $./arbres-paris.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2 - Create the spark session variable, name it \"preprocessing\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:39:33.485400Z",
     "start_time": "2020-03-05T07:38:31.989417Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import findspark\n",
    "findspark.init('C:/Spark/spark-2.4.4-bin-hadoop2/spark-2.4.4-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "### STRIP_START\n",
    "\n",
    "try:\n",
    "    os.remove(\"metastore_db/db.lck\")\n",
    "    os.remove(\"metastore_db/dbex.lck\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def build_spark_session(app_name, memory='4g', executors=4):\n",
    "    return SparkSession.builder\\\n",
    "                      .appName(app_name)\\\n",
    "                      .config('spark.executor.memory', memory)\\\n",
    "                      .config('spark.executor.instances', executors)\\\n",
    "                      .getOrCreate()\n",
    "\n",
    "spark_session = build_spark_session(app_name='ok-google')\n",
    "### STRIP_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & exploring data\n",
    "\n",
    "**Q3 - Load the data from the train_clean.csv. We have seen at Q1 that this file has a header.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T22:07:40.432231Z",
     "start_time": "2020-03-04T22:07:40.169239Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex‚cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "filename = \"./train_clean.csv\"\n",
    "!head - n 2 $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:40:07.381406Z",
     "start_time": "2020-03-05T07:39:33.504406Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "df = spark_session.read.csv(filename, sep=\",\", \\\n",
    "                     header=True, \\\n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T22:08:59.338248Z",
     "start_time": "2020-03-04T22:08:59.233235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BP5NDTJ.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ok-google</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=ok-google>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:26:29.560038Z",
     "start_time": "2020-03-04T16:26:29.552041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project_id',\n",
       " 'name',\n",
       " 'desc',\n",
       " 'goal',\n",
       " 'keywords',\n",
       " 'disable_communication',\n",
       " 'country',\n",
       " 'currency',\n",
       " 'deadline',\n",
       " 'state_changed_at',\n",
       " 'created_at',\n",
       " 'launched_at',\n",
       " 'backers_count',\n",
       " 'final_status']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4 - Let's go for some exploration :**\n",
    "\t- 4.1) Number of lines and columns.\n",
    "\t- 4.2) Display the first 20  rows of the dataframe.\n",
    "\t- 4.3) Print the schema of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:14:07.819051Z",
     "start_time": "2020-03-04T16:14:07.228045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108129"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4.1 Number of lines and columns.\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:28:21.901040Z",
     "start_time": "2020-03-04T16:28:21.514096Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+-------+--------------------+---------------------+-------+--------+----------+----------------+----------+-----------+-------------+------------+\n",
      "|    project_id|                name|                desc|   goal|            keywords|disable_communication|country|currency|  deadline|state_changed_at|created_at|launched_at|backers_count|final_status|\n",
      "+--------------+--------------------+--------------------+-------+--------------------+---------------------+-------+--------+----------+----------------+----------+-----------+-------------+------------+\n",
      "|kkst1451568084| drawing for dollars|I like drawing pi...|   20.0| drawing-for-dollars|                False|     US|     USD|1241333999|      1241334017|1240600507| 1240602723|            3|           1|\n",
      "|kkst1474482071|Sponsor Dereck Bl...|I  Dereck Blackbu...|  300.0|sponsor-dereck-bl...|                False|     US|     USD|1242429000|      1242432018|1240960224| 1240975592|            2|           0|\n",
      "| kkst183622197|       Mr. Squiggles|So I saw darkpony...|   30.0|        mr-squiggles|                False|     US|     USD|1243027560|      1243027818|1242163613| 1242164398|            0|           0|\n",
      "| kkst597742710|Help me write my ...|Do your part to h...|  500.0|help-me-write-my-...|                False|     US|     USD|1243555740|      1243556121|1240963795| 1240966730|           18|           1|\n",
      "|kkst1913131122|Support casting m...|I m nearing compl...| 2000.0|support-casting-m...|                False|     US|     USD|1243769880|      1243770317|1241177914| 1241180541|            1|           0|\n",
      "|kkst1085176748|        daily digest|I m a fledgling v...|  700.0|        daily-digest|                False|     US|     USD|1243815600|      1243816219|1241050799| 1241464468|           14|           0|\n",
      "|kkst1468954715|iGoozex - Free iP...|I am an independe...|  250.0|igoozex-free-ipho...|                False|     US|     USD|1243872000|      1243872028|1241725172| 1241736308|            2|           0|\n",
      "| kkst194050612|Drive A Faster Ca...|Drive A Faster Ca...| 1000.0|drive-a-faster-ca...|                False|     US|     USD|1244088000|      1244088022|1241460541| 1241470291|           32|           1|\n",
      "| kkst708883590|\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...|Opening Friday  J...| 5000.0|lostles-at-tinys-...|                False|     US|     USD|1244264400|      1244264422|1241415164| 1241480901|           44|           0|\n",
      "| kkst890976740|Choose Your Own A...|This project is f...| 3500.0|choose-your-own-a...|                False|     US|     USD|1244946540|      1244946632|1242268157| 1242273460|           18|           0|\n",
      "|kkst2053381363|Anatomy of a Cred...|I am an independe...|30000.0|anatomy-of-a-cred...|                False|     US|     USD|1245026160|      1245026721|1241829376| 1242056094|            7|           0|\n",
      "| kkst918550886|No-bit: An artist...|I want to create ...|  300.0|no-bit-an-artist-...|                False|     US|     USD|1245038400|      1245038428|1242523061| 1242528805|            2|           0|\n",
      "| kkst934689279|Indie Nerd Board ...|pictured here is ...| 1500.0|indie-nerd-board-...|                False|     US|     USD|1245042600|      1245042919|1242364202| 1242369560|           28|           1|\n",
      "| kkst191414809|Icons for your iP...|I make cool icons...|  500.0|awesome-icons-for...|                False|     US|     USD|1245092400|      1245092431|1241034764| 1241039475|           98|           1|\n",
      "| kkst569584443|HAPPY VALLEY: Dex...|I am a profession...|  500.0|help-me-make-my-w...|                False|     US|     USD|1245528660|      1245528920|1242072711| 1242333869|            3|           0|\n",
      "| kkst485555421|       Project Pedal|Project Pedal is ...| 1000.0|       project-pedal|                False|     US|     USD|1245556740|      1245556829|1242682134| 1242690018|           20|           1|\n",
      "|kkst1537563608|Frank Magazine Er...|We are throwing a...|  600.0|frank-magazine-er...|                False|     US|     USD|1245882360|      1245882631|1244579167| 1244742156|           12|           0|\n",
      "|kkst1261713500|  Crossword Puzzles!|I create crosswor...| 1500.0|   crossword-puzzles|                False|     US|     USD|1246354320|      1246355121|1240997554| 1241005923|          163|           1|\n",
      "| kkst910550425|Run, Blago Run! Show|A 3-day pop-up ar...| 3500.0|  run-blago-run-show|                False|     US|     USD|1246420800|      1246420854|1244299453| 1244388012|           54|           0|\n",
      "| kkst139451001|It Might Become a...|We are broke film...| 1000.0|it-might-become-a...|                False|     US|     USD|1246420800|      1246420840|1243272026| 1243616180|           23|           1|\n",
      "+--------------+--------------------+--------------------+-------+--------------------+---------------------+-------+--------+----------+----------------+----------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4.2 Display the first 20  rows of the dataframe.\n",
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T22:09:55.573236Z",
     "start_time": "2020-03-04T22:09:55.560240Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: string (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- disable_communication: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- deadline: string (nullable = true)\n",
      " |-- state_changed_at: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- launched_at: string (nullable = true)\n",
      " |-- backers_count: integer (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4.3 Print the schema of the dataframe.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:26:43.646046Z",
     "start_time": "2020-03-04T16:26:43.628044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project_id', 'string'),\n",
       " ('name', 'string'),\n",
       " ('desc', 'string'),\n",
       " ('goal', 'string'),\n",
       " ('keywords', 'string'),\n",
       " ('disable_communication', 'string'),\n",
       " ('country', 'string'),\n",
       " ('currency', 'string'),\n",
       " ('deadline', 'string'),\n",
       " ('state_changed_at', 'string'),\n",
       " ('created_at', 'string'),\n",
       " ('launched_at', 'string'),\n",
       " ('backers_count', 'int'),\n",
       " ('final_status', 'int')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5 - When printing the schema, we see that all columns are strings. Assign the integer type to columns you think appropriate. Have a look at the csv file. This new dataframe will be named dfCasted, print its schema.**\n",
    "\n",
    "*Hint : Use the .withColumn(newColName, newColValue) to cast each column.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:51:50.376407Z",
     "start_time": "2020-03-05T07:51:50.271407Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "dfCasted = df.withColumn(\"goal\", df[\"goal\"].cast(\"integer\"))\\\n",
    "             .withColumn(\"deadline\", df[\"deadline\"].cast(\"integer\"))\\\n",
    "             .withColumn(\"state_changed_at\", df[\"state_changed_at\"].cast(\"integer\"))\\\n",
    "             .withColumn(\"created_at\", df[\"created_at\"].cast(\"integer\"))\\\n",
    "             .withColumn(\"launched_at\", df[\"launched_at\"].cast(\"integer\"))\\\n",
    "             .withColumn(\"gobackers_countal\", df[\"backers_count\"].cast(\"integer\"))\\\n",
    "             .withColumn(\"final_status\", df[\"final_status\"].cast(\"integer\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T22:10:08.688232Z",
     "start_time": "2020-03-04T22:10:08.678237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- disable_communication: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- deadline: integer (nullable = true)\n",
      " |-- state_changed_at: integer (nullable = true)\n",
      " |-- created_at: integer (nullable = true)\n",
      " |-- launched_at: integer (nullable = true)\n",
      " |-- backers_count: integer (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- gobackers_countal: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "dfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following result : \n",
    "\n",
    "```\n",
    "root\n",
    " |-- project_id: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    " |-- desc: string (nullable = true)\n",
    " |-- goal: integer (nullable = true)\n",
    " |-- keywords: string (nullable = true)\n",
    " |-- disable_communication: string (nullable = true)\n",
    " |-- country: string (nullable = true)\n",
    " |-- currency: string (nullable = true)\n",
    " |-- deadline: integer (nullable = true)\n",
    " |-- state_changed_at: integer (nullable = true)\n",
    " |-- created_at: integer (nullable = true)\n",
    " |-- launched_at: integer (nullable = true)\n",
    " |-- backers_count: integer (nullable = true)\n",
    " |-- final_status: integer (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6 - We could have done this much faster. Do you know how ?**\n",
    "\n",
    "**Hint** : Have a look at parameters in cell n°4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:29:13.237040Z",
     "start_time": "2020-03-04T16:29:12.913046Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|goal|\n",
      "+----+\n",
      "|  20|\n",
      "| 300|\n",
      "|  30|\n",
      "+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO : Write your answer\n",
    "dfCasted.select(\"goal\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "**Q7 - Give a statistical description of these columns together : goal, backers_count, final_status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:30:38.709880Z",
     "start_time": "2020-03-04T16:30:33.776884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+-------------------+\n",
      "|summary|             goal|      backers_count|       final_status|\n",
      "+-------+-----------------+-------------------+-------------------+\n",
      "|  count|           107615|             108128|             108128|\n",
      "|   mean|36839.03430748502|  6434187.413250962| 1052360.7834973366|\n",
      "| stddev|974215.3015529736|9.324061726649426E7|3.776049940184165E7|\n",
      "|    min|                0|                  0|                  0|\n",
      "|    max|        100000000|         1430423170|         1428977971|\n",
      "+-------+-----------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "dfCasted.select(\"goal\", \"backers_count\",\"final_status\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8 - Let's have a look at the **disable_communication** column. Group by values and display a descending value count. Show the top 10 values.**\n",
    "\n",
    "*Hint : groupBy, count, orderBy, show*\n",
    "\n",
    "What do you notice ? Considering the number of lines of our dataset, does this column provides information ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:52:48.961887Z",
     "start_time": "2020-03-04T16:52:45.640889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+\n",
      "|disable_communication| count|\n",
      "+---------------------+------+\n",
      "|                False|107293|\n",
      "|                 True|   322|\n",
      "|               2500.0|     8|\n",
      "|               1000.0|     7|\n",
      "|               5000.0|     6|\n",
      "|              10000.0|     5|\n",
      "|               2000.0|     4|\n",
      "|               8000.0|     3|\n",
      "|  The Artist s Pro...|     3|\n",
      "|               3000.0|     3|\n",
      "+---------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCasted.groupBy(\"disable_communication\").count().sort(f.desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:35:09.535888Z",
     "start_time": "2020-03-04T16:35:09.462880Z"
    }
   },
   "source": [
    "df.groupBy(\"disable_communication\").count().orderBy('count', ascending=False).sorthow(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What shall you do with this **disable_communication** column ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:51:57.567403Z",
     "start_time": "2020-03-05T07:51:57.553409Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "dfCasted = dfCasted.drop(\"disable_communication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9 - Houston, we have a problem ! We can see the future in our dataset ! Can you find it ? These informations must be removed.**\n",
    "\n",
    "*Hint : There are two problematic columns, it has something to do with the supporters, and a change during the project.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T14:33:10.673299Z",
     "start_time": "2019-05-29T14:33:10.528318Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q10 - Country & Currency : Start with some exploration of these columns.**\n",
    "\n",
    "- Try some groupBy and counting, just like *Q8*. Then, read below.\n",
    "\n",
    "You may think that *country* and *currency* are redundant, in which case we could just delete one of the two columns. What about Euro ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:05:16.475127Z",
     "start_time": "2020-03-04T17:05:14.553128Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|country|count|\n",
      "+-------+-----+\n",
      "|     US|91545|\n",
      "|     GB| 8743|\n",
      "|     CA| 3733|\n",
      "|     AU| 1877|\n",
      "|     NL|  702|\n",
      "|  False|  428|\n",
      "|     NZ|  354|\n",
      "|     SE|  240|\n",
      "|     DK|  196|\n",
      "|     NO|  113|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO : Country value count\n",
    "#dfCasted.count()\n",
    "dfCasted.groupBy(\"country\").count().sort(f.desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:53:05.334896Z",
     "start_time": "2020-03-04T16:53:03.227891Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|currency|count|\n",
      "+--------+-----+\n",
      "|     USD|91545|\n",
      "|     GBP| 8743|\n",
      "|     CAD| 3733|\n",
      "|     AUD| 1877|\n",
      "|     EUR|  814|\n",
      "|      US|  406|\n",
      "|     NZD|  354|\n",
      "|     SEK|  240|\n",
      "|     DKK|  196|\n",
      "|     NOK|  113|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO : Currency value count\n",
    "#dfCasted.count()\n",
    "dfCasted.groupBy(\"currency\").count().sort(f.desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:49:56.627878Z",
     "start_time": "2020-03-04T16:49:56.209888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108129"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCasted.select(\"currency\", \"country\").count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:50:20.438887Z",
     "start_time": "2020-03-04T16:50:20.429883Z"
    }
   },
   "source": [
    "dfCasted = dfCasted.drop(\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try selecting *goal* and *final_status*, and show some values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T16:51:14.927883Z",
     "start_time": "2020-03-04T16:51:14.648887Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|goal|final_status|\n",
      "+----+------------+\n",
      "|  20|           1|\n",
      "| 300|           0|\n",
      "|  30|           0|\n",
      "| 500|           1|\n",
      "|2000|           0|\n",
      "+----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "dfCasted.select(\"goal\", \"final_status\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try showing value count for country and currency in the same table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T18:22:48.666139Z",
     "start_time": "2020-03-04T18:22:11.669140Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+\n",
      "|currency|country|count|\n",
      "+--------+-------+-----+\n",
      "|     USD|     US|91545|\n",
      "|     GBP|     GB| 8743|\n",
      "|     CAD|     CA| 3733|\n",
      "|     AUD|     AU| 1877|\n",
      "|     EUR|     NL|  702|\n",
      "|      US|  False|  405|\n",
      "|     NZD|     NZ|  354|\n",
      "|     SEK|     SE|  240|\n",
      "|     DKK|     DK|  196|\n",
      "|     NOK|     NO|  113|\n",
      "+--------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "#dfCasted.select(\"country\", \"currency\").distinct().show()\n",
    "#df.agg(*(f.count(f.col(c)).alias(c) for c in [\"country\", \"currency\"])).show()\n",
    "dfCasted.groupBy(\"currency\",\"country\").count().sort(f.desc(\"count\")).show(10)\n",
    "#A rédiger le sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11 - Now, there is something else : Some values for *country* have the value *False*. Display these records, and groupBy *currency*, descending.**\n",
    "\n",
    "*Hint : The instruction chain is the following : dfCasted.filter().groupBy().count().orderBy().show(), fill 3 parentheses.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T22:12:39.896229Z",
     "start_time": "2020-03-04T22:12:31.662263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------\n",
      " currency | NZ  \n",
      " count    | 1   \n",
      "-RECORD 1-------\n",
      " currency | NO  \n",
      " count    | 1   \n",
      "-RECORD 2-------\n",
      " currency | NL  \n",
      " count    | 2   \n",
      "-RECORD 3-------\n",
      " currency | AU  \n",
      " count    | 3   \n",
      "-RECORD 4-------\n",
      " currency | CA  \n",
      " count    | 3   \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "dfCasted.filter(f.col(\"country\")==\"False\").groupBy(\"currency\").count().sort(f.desc(\"count\")).show(5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Definition - Custom functions :* Some column operations are already defined inside Spark, but we often need to apply more complex or more custom function. In this case, we can create User Defined Functions (UDF) and apply them on columns.\n",
    "\n",
    "**Q12 - In this question, we will create two UDF.**\n",
    "- **udfCountry(country, currency)** : If country=False, take the currency value, else, leave the country value.\n",
    "- **udfCurrency(currency)** : If the length of currency is different than 3, assign a null value, else, leave the currency value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:41:43.463401Z",
     "start_time": "2020-03-05T07:41:43.456409Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:41:47.247432Z",
     "start_time": "2020-03-05T07:41:47.225404Z"
    }
   },
   "outputs": [],
   "source": [
    "def f_country(country:str, currency:str) -> bool:\n",
    "    return currency if country is False else country\n",
    "from typing import Optional\n",
    "def f_currency(currency:str)->Optional:\n",
    "    return currency if len(currency)==3 else Null\n",
    "udf_country = f.udf(f_country, StringType())\n",
    "udf_currency = f.udf(f_currency, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q16\n",
    "\n",
    "df = df\\\n",
    "     .withColumn('hours_prepa', datediff(col('dead_line_clean'), coll('lauched_at_clean')))\\\n",
    "     .#round au lieu datediff 3600,2 hours_prepa diff\n",
    "def filter_udf(input_prepa, days_campagns, goal)->bool:\n",
    "    def _gt(value)->booll:\n",
    "        try return value==0\n",
    "    exept:\n",
    "        return False\n",
    "    return _gt(input_prepa and _gt(days_campagns))\n",
    "\n",
    "udf_filter = f.udf(filter_udf, BooleanType())\n",
    "\n",
    "df.filter(udf_filter(col('hours_prepa'), col('days_campagns'), col('goal')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q24\n",
    "df\\\n",
    ".repartition(8*10)\n",
    ".write\\\n",
    ".mode(overwrite)\n",
    ".paquet(paquetquet_file_path)#fichier paquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T18:22:10.323139Z",
     "start_time": "2020-03-04T18:22:09.883136Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-43d1180fa5b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#check the input output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minput_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_countut_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0moutput_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_countut_pat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "#check the input output\n",
    "input_count = spark.read.csv(input_countut_path).count()\n",
    "output_count = spark.read.csv(input_countut_pat).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BinaryType – Données binaires.\n",
    "- BooleanType – Valeurs booléennes.\n",
    "- ByteType – Valeur d'octet.\n",
    "- DateType – Valeur d'horodatage.\n",
    "- DoubleType – Valeur double à virgule flottante.\n",
    "- IntegerType – Valeur d'entier.\n",
    "- LongType – Valeur d'entier long.\n",
    "- NullType – Valeur null.\n",
    "- ShortType – Valeur d'entier court.\n",
    "- StringType – Chaîne de texte.\n",
    "- TimestampType – Valeur d'horodatage (généralement en secondes à partir du 01/01/1970).\n",
    "- UnknownType – Valeur de type non identifié."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13 - In this question we will apply our two UDF. Using the .withColumn operation, you can change a column, just like you did for type casting. withColumn will create two new columns : country2 and currency2.**\n",
    "\n",
    "*Hint : df.withcolumn(country2, newColValue).withcolumn(currency2, newColValue)*\n",
    "\n",
    "Also, you can add a drop statement (on country and currency) after the two withColumns, as we have created our new columns.\n",
    "\n",
    "Check your dataframe once transformations are applied. Schema and first lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:52:18.031406Z",
     "start_time": "2020-03-05T07:52:17.961429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- deadline: integer (nullable = true)\n",
      " |-- state_changed_at: integer (nullable = true)\n",
      " |-- created_at: integer (nullable = true)\n",
      " |-- launched_at: integer (nullable = true)\n",
      " |-- backers_count: integer (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- gobackers_countal: integer (nullable = true)\n",
      " |-- country2: string (nullable = true)\n",
      " |-- currency2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " dfCasted = dfCasted.withColumn(\"country2\", udf_country(f.col(\"country\"),f.col(\"currency\")))\\\n",
    "   .withColumn(\"currency2\", udf_currency(f.col(\"currency\")))\\\n",
    "   .drop(\"country\", \"currency\")\n",
    "dfCasted.printSchema()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T22:30:56.500325Z",
     "start_time": "2020-03-04T22:30:54.353346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|country2|currency2|\n",
      "+--------+---------+\n",
      "|      US|      USD|\n",
      "|      US|      USD|\n",
      "|      US|      USD|\n",
      "|      US|      USD|\n",
      "|      US|      USD|\n",
      "+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCasted.select(\"country2\", \"currency2\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14 - We will do one more cleanup on the column final_status**, which will be the label for our classification algorithm in next course.\n",
    "\n",
    "First, count the number of elements for each values in final_status.\n",
    "\n",
    "Finally, we need to delete records with **final_status** different than 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T22:33:37.196329Z",
     "start_time": "2020-03-04T22:33:36.291331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108129"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO : final_status count\n",
    "dfCasted.select(\"final_status\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T22:38:41.758338Z",
     "start_time": "2020-03-04T22:38:40.068340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34419, 73266)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCasted.filter(f.col(\"final_status\")==1).count(),dfCasted.filter(f.col(\"final_status\")==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T22:37:27.602329Z",
     "start_time": "2020-03-04T22:37:27.074349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|final_status|\n",
      "+------------+\n",
      "|           1|\n",
      "|           0|\n",
      "|           0|\n",
      "|           1|\n",
      "|           0|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO : filter\n",
    "dfCasted.filter((f.col(\"final_status\")==1)|(f.col(\"final_status\")==0))\\\n",
    "        .select(\"final_status\")\\\n",
    "        .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T09:23:45.598338Z",
     "start_time": "2019-05-24T09:23:44.718137Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : Check processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:52:36.988408Z",
     "start_time": "2020-03-05T07:52:36.923399Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_valid_label(label:int)->bool:\n",
    "    return label in [0,1]\n",
    "udf_is_valide = f.udf(is_valid_label, BooleanType())\n",
    "dfCasted = dfCasted.filter(udf_is_valide(f.col('final_status')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T22:56:52.366341Z",
     "start_time": "2020-03-04T22:56:50.544388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|final_status|\n",
      "+------------+\n",
      "|           1|\n",
      "|           0|\n",
      "|           0|\n",
      "|           1|\n",
      "|           0|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCasted.select('final_status').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "It's sometimes useful to add features to our dataframe, to help our model learning. We will work with the time data.\n",
    "\n",
    "**Q15 - Our dates columns are in unix timestamps. We first need to convert them to dates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T23:02:09.412328Z",
     "start_time": "2020-03-04T23:02:09.401332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- deadline: integer (nullable = true)\n",
      " |-- state_changed_at: integer (nullable = true)\n",
      " |-- created_at: integer (nullable = true)\n",
      " |-- launched_at: integer (nullable = true)\n",
      " |-- backers_count: integer (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- gobackers_countal: integer (nullable = true)\n",
      " |-- country2: string (nullable = true)\n",
      " |-- currency2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T23:27:05.191339Z",
     "start_time": "2020-03-04T23:27:03.258340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------+-----------+\n",
      "|  deadline|state_changed_at|created_at|launched_at|\n",
      "+----------+----------------+----------+-----------+\n",
      "|1241333999|      1241334017|1240600507| 1240602723|\n",
      "|1242429000|      1242432018|1240960224| 1240975592|\n",
      "+----------+----------------+----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCasted.select(\"deadline\",\"state_changed_at\",\"created_at\", \"launched_at\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:52:46.890406Z",
     "start_time": "2020-03-05T07:52:44.497407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------+-----------+\n",
      "|  deadline|state_changed_at|created_at|launched_at|\n",
      "+----------+----------------+----------+-----------+\n",
      "|2009-05-03|      2009-05-03|2009-04-24| 2009-04-24|\n",
      "|2009-05-15|      2009-05-16|2009-04-28| 2009-04-29|\n",
      "|2009-05-22|      2009-05-22|2009-05-12| 2009-05-12|\n",
      "|2009-05-29|      2009-05-29|2009-04-29| 2009-04-29|\n",
      "|2009-05-31|      2009-05-31|2009-05-01| 2009-05-01|\n",
      "+----------+----------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from pyspark.sql.functions import to_date\n",
    "dfCasted = dfCasted.withColumn(\"deadline\",f.from_unixtime('deadline').cast(DateType()) )\\\n",
    "        .withColumn(\"state_changed_at\",f.from_unixtime('state_changed_at').cast(DateType()) )\\\n",
    "        .withColumn(\"created_at\",f.from_unixtime('created_at').cast(DateType()) )\\\n",
    "        .withColumn(\"launched_at\",f.from_unixtime('launched_at').cast(DateType())) \\\n",
    "\n",
    "dfCasted.select(\"deadline\",\"state_changed_at\",\"created_at\", \"launched_at\")\\\n",
    "        .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q16 - Add a **days_campaign** column, which represents the duration of the campaign, in days. This is the difference between *launched_at* and *deadline*. Here we work with a date difference.**\n",
    "\n",
    "Add a **hours_prep**, which represents the number of hours of preparation. This is the difference between *created_at* and *launched_at*. You may round to 2 digits after comma. Here we work with a timestamp difference.\n",
    "\n",
    "Finally, apply a filter : we want to delete records with **days_campaign** AND **hours_prep** equal to zero, and we want the records with **goal** greater than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\\\n",
    "     .withColumn('hours_prepa', datediff(col('dead_line_clean'), coll('lauched_at_clean')))\\\n",
    "     .#round au lieu datediff 3600,2 hours_prepa diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:52:54.600402Z",
     "start_time": "2020-03-05T07:52:54.566408Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- deadline: date (nullable = true)\n",
      " |-- state_changed_at: date (nullable = true)\n",
      " |-- created_at: date (nullable = true)\n",
      " |-- launched_at: date (nullable = true)\n",
      " |-- backers_count: integer (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- gobackers_countal: integer (nullable = true)\n",
      " |-- country2: string (nullable = true)\n",
      " |-- currency2: string (nullable = true)\n",
      " |-- days_campaign: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round, datediff\n",
    "\n",
    "# Date difference : datediff(Col1, Col2)\n",
    "dfCasted = dfCasted.withColumn(\"days_campaign\",datediff(f.col(\"deadline\"),f.col(\"launched_at\")))\n",
    "dfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T07:56:27.051406Z",
     "start_time": "2020-03-05T07:56:27.040401Z"
    }
   },
   "outputs": [],
   "source": [
    "?f.round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:02:18.298402Z",
     "start_time": "2020-03-05T08:02:18.206404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- deadline: date (nullable = true)\n",
      " |-- state_changed_at: date (nullable = true)\n",
      " |-- created_at: date (nullable = true)\n",
      " |-- launched_at: date (nullable = true)\n",
      " |-- backers_count: integer (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- gobackers_countal: integer (nullable = true)\n",
      " |-- country2: string (nullable = true)\n",
      " |-- currency2: string (nullable = true)\n",
      " |-- days_campaign: integer (nullable = true)\n",
      " |--  hours_prep: integer (nullable = true)\n",
      " |-- hours_prep: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCasted = dfCasted.withColumn(\"hours_prep\",f.round(datediff(f.col(\"launched_at\"),f.col(\"created_at\")),2))\n",
    "dfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:06:34.742408Z",
     "start_time": "2020-03-05T08:06:21.614403Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+--------+--------+----------------+----------+-----------+-------------+------------+-----------------+--------+---------+-------------+-----------+----------+\n",
      "|project_id|name|desc|goal|keywords|deadline|state_changed_at|created_at|launched_at|backers_count|final_status|gobackers_countal|country2|currency2|days_campaign| hours_prep|hours_prep|\n",
      "+----------+----+----+----+--------+--------+----------------+----------+-----------+-------------+------------+-----------------+--------+---------+-------------+-----------+----------+\n",
      "+----------+----+----+----+--------+--------+----------------+----------+-----------+-------------+------------+-----------------+--------+---------+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO : Filter\n",
    "#Finally, apply a filter : we want to delete records with days_campaign AND hours_prep equal to zero, and we want the records with goal greater than zero\n",
    "\n",
    "dfCasted.filter((f.col(\"days_campaign\")==0)&(f.col(\"hours_prep\")==0)&(f.col(\"goal\")>0)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q17 - At this point, we don't need these columns anymore : *created_at*, *launched_at*, *deadline*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T10:31:15.467978Z",
     "start_time": "2019-05-24T10:31:15.449766Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : Drops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now work on text data, we will gather every text values into one.\n",
    "\n",
    "**Q18 - Pass the columns *name*, *desc* and *keywords* into lowercase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T10:31:16.088184Z",
     "start_time": "2019-05-24T10:31:16.054039Z"
    }
   },
   "outputs": [],
   "source": [
    "# A little search for passing strings to lower case ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q19 - Create a new column called *text* which contains the three previous columns. Be careful to include a space between them so that we can split them later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Hint : Google(\"pyspark concat_ws\"), don't forget the separator parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q20 - You can now delete these three text columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T10:31:18.118153Z",
     "start_time": "2019-05-24T10:31:18.101652Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing null values\n",
    "\n",
    "**Q21 - There are various techniques to handle null values to make them usable by an algorithm. Can you find 3 different methods ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Write your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q22 - For the columns *days_campaign*, *hours_prep* and *goal* : replace null values by **-1**.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T10:37:16.344695Z",
     "start_time": "2019-05-24T10:37:16.302203Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look for na.fill at the following adress :\n",
    "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q23 - For the columns **country2** and **currency2** : replace null values by **\"unknown\"**.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Dataframe\n",
    "\n",
    "Well done, you have done a pretty good pipeline for pre-processing your dataset.\n",
    "\n",
    "**Q24 - Finally, export your dataframe to the *parquet* format.**\n",
    "\n",
    "*parquet* always exports a folder that may contain multiple files, this is due to the distributed nature of Spark.\n",
    "\n",
    "The export function creates a directory with the name given in parameter. Give it **\"kickstarter.parquet\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T10:40:48.532087Z",
     "start_time": "2019-05-24T10:40:45.353047Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
